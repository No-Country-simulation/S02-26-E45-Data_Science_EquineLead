# ğŸ“Š Registro de Experimentos con MLflow en DagsHub

Este tutorial explica **cÃ³mo levantar el entorno con `uv` y cÃ³mo registrar experimentos en MLflow usando los templates del proyecto**. EstÃ¡ pensado para Data Scientists del equipo EquineLead.

> âš ï¸ **Requisito obligatorio**: para poder registrar experimentos en MLflow **debÃ©s ser colaborador del repositorio en DagsHub**. Si no lo sos, vas a ver errores de permisos (401/403).

---

## 1ï¸âƒ£ Levantar el entorno de trabajo (uv)

âš ï¸ **Importante**: el entorno **NO se construye de cero**. Este repositorio ya incluye:

* `pyproject.toml`
* `uv.lock`

Estos archivos garantizan un entorno **reproducible y estandarizado para todo el equipo**.

### Paso 1 â€“ Instalar uv (si no lo tenÃ©s)

```bash
pip install uv
```

### Paso 2 â€“ Crear el entorno virtual y sincronizar dependencias a partir del lockfile (reproducible)

```bash
uv sync --locked
```

ğŸ“Œ Este comando instala **exactamente** las versiones fijadas en `uv.lock`.

---

## 2ï¸âƒ£ DÃ³nde viven los experimentos

Los experimentos de ML estÃ¡n organizados en:

```text
src/experiments/
â”œâ”€â”€ engine/
â”œâ”€â”€ leads_horses/
â””â”€â”€ leads_products/
```

Cada carpeta representa **un experimento de negocio distinto**.

Dentro de cada experimento hay siempre los mismos templates:

```text
features.py   # Feature engineering
model.py      # Entrenamiento del modelo
metrics.py    # MÃ©tricas
train.py      # Orquestador + MLflow
```

âš ï¸ **Regla del equipo**: los DS solo modifican estos archivos. No tocar `misc/`, `flows/` ni infra.

---

## 3ï¸âƒ£ QuÃ© tenÃ©s que modificar como Data Scientist

### ğŸ§  `train.py` â€“ ConfiguraciÃ³n personal (OBLIGATORIO)

En la parte superior del archivo:

```python
# ==================================
# DATA SCIENTIST PERSONAL CONFIG
# ==================================
RUN_NAME = "baseline_xgboost_v1_YYYYMMDD_HHMMSS"
DS_NAME = "Tu_Nombre_Apellido"
STAGE = "training"
```

ğŸ”§ **DebÃ©s cambiar**:

* `RUN_NAME`: nombre descriptivo del experimento
* `DS_NAME`: tu nombre (queda auditado en MLflow)
* `STAGE`: normalmente `training` (no cambiar salvo indicaciÃ³n del MLE)

---

### ğŸ“¦ Dataset

```python
PATH_DATA = Path("./data/clean")
DATASET_NAME = "dataset_name.parquet"
```

ğŸ”§ Cambiar solo si usÃ¡s otro dataset **aprobado**.

---

## 4ï¸âƒ£ Feature Engineering â€“ `features.py`

En este archivo se define **quÃ© datos entran al modelo y cÃ³mo se construyen**.

### QuÃ© se debe modificar

BuscÃ¡ la funciÃ³n principal (por ejemplo `build_features`) y **modificÃ¡ solo estas secciones**:

1. **SelecciÃ³n de columnas (features y target)**

   ```python
   features = [
       "col_1",
       "col_2",
       "col_3",
   ]

   target = "target_column"
   ```

2. **Transformaciones**

   ```python
   df_features = df[features]
   # acÃ¡ podÃ©s:
   # - crear features nuevas
   # - normalizar / escalar
   # - encodear variables categÃ³ricas
   ```

3. **Split de datos**

   ```python
   X_train, X_val, y_train, y_val = train_test_split(
       df_features,
       df[target],
       test_size=0.2,
       random_state=random_state,
   )
   ```

ğŸš« **No modificar**:

* Imports compartidos
* Firma de la funciÃ³n
* El return (`X_train, X_val, y_train, y_val`)

ğŸ“Œ El objetivo es que `train.py` no cambie nunca.

---

## 5ï¸âƒ£ Entrenamiento â€“ `model.py`

Ejemplo actual:

```python
model = RandomForestRegressor(
    n_estimators=200,
    max_depth=10,
    random_state=random_state,
    n_jobs=-1
)
```

ğŸ”§ **Modificar cuando**:

* CambiÃ¡s el algoritmo
* AjustÃ¡s hiperparÃ¡metros

ğŸ“Œ **Regla clave**:

* Scikit-learn / wrappers â†’ `mlflow.sklearn.log_model`
* DL / frameworks nativos â†’ logger especÃ­fico

---

## 6ï¸âƒ£ MÃ©tricas â€“ `metrics.py`

Ejemplo:

```python
return {
    "rmse": root_mean_squared_error(y_val, preds),
    "mae": mean_absolute_error(y_val, preds),
}
```

ğŸ”§ **AcÃ¡ es donde tenÃ©s que modificar si usÃ¡s**:

* Otras mÃ©tricas
* ClasificaciÃ³n (accuracy, f1, auc, etc.)
* MÃ©tricas custom

ğŸ“Œ Todo lo que devuelva este diccionario se registra automÃ¡ticamente en MLflow:

```python
for k, v in metrics.items():
    mlflow.log_metric(k, v)
```

---

## 7ï¸âƒ£ Registro de Dataset (OBLIGATORIO)

En `train.py`:

```python
log_dataset_metadata(
    name="horses_listings",
    version="v1.0.1",
    path="/clean/horses_listings_limpio.parquet",
    n_rows=df.shape[0],
    n_cols=df.shape[1],
)
```

ğŸ”§ **DebÃ©s cambiar**:

* `name`
* `version`
* `path`

ğŸ“Œ Esto es clave para trazabilidad y auditorÃ­a.

---

## 8ï¸âƒ£ Reproducibilidad y entorno (NO TOCAR)

```python
mlflow.log_param("random_state", SEED)
mlflow.log_param("python_version", sys.version)
mlflow.log_param("os", platform.system())
```

Esto garantiza que el experimento sea reproducible.

---

## 9ï¸âƒ£ InformaciÃ³n del modelo (OBLIGATORIO)

En `train.py`:

```python
# =====================
# Model Info
# =====================
mlflow.log_param("model_type", model.__class__.__name__)
mlflow.log_param("model_family", "tree-based")
```

### QuÃ© debÃ©s modificar

* `model_type`: **se completa automÃ¡ticamente**, no tocar.
* `model_family`: **DEBÃ‰S ajustarlo segÃºn el modelo usado**.

Ejemplos vÃ¡lidos:

| Tipo de modelo               | model_family    |
| ---------------------------- | --------------- |
| RandomForest, XGBoost        | `tree-based`    |
| Linear / Logistic Regression | `linear`        |
| Neural Networks              | `deep-learning` |
| Reglas / heurÃ­sticas         | `rule-based`    |
| Modelos custom               | `custom`        |

ğŸ“Œ Esto se usa luego para:

* Filtrar experimentos
* Automatizar despliegues
* AnÃ¡lisis comparativos

---

## ğŸ”Ÿ Registrar el modelo

Ejemplo actual:

```python
mlflow.sklearn.log_model(
    model,
    artifact_path="model_engine"
)
```

ğŸ“Œ **Usar el logger correcto segÃºn el framework**:

* sklearn / XGBoost wrapper â†’ `mlflow.sklearn.log_model`
* PyTorch â†’ `mlflow.pytorch.log_model`
* TensorFlow â†’ `mlflow.tensorflow.log_model`
* Custom â†’ `mlflow.pyfunc.log_model`

ğŸš« No mezclar loggers.

---

## ğŸ”Ÿ Ejecutar el experimento

Desde la raÃ­z del repo:

```bash
python src/experiments/engine/train.py
```

Si todo estÃ¡ bien:

* El run aparece en MLflow (DagsHub)
* Quedan registrados:

  * MÃ©tricas
  * ParÃ¡metros
  * Dataset
  * Modelo

---

## ğŸš¨ Errores comunes

### âŒ No puedo registrar experimentos

âœ” VerificÃ¡ que seas **colaborador del repo en DagsHub**

### âŒ Error de credenciales

âœ” RevisÃ¡ `.env` y configuraciÃ³n de MLflow

---

## â­ Best Practices del equipo (OBLIGATORIAS)

* Usar nombres de runs descriptivos y comparables
* Un cambio conceptual = un run nuevo
* No sobreescribir mÃ©tricas existentes
* Loguear **SIEMPRE** dataset + versiÃ³n
* Mantener consistencia en `model_family`
* Todo experimento debe ser reproducible con `uv sync --locked`

---

## âœ… Checklist final para DS (OBLIGATORIO)

Antes de dar por vÃ¡lido un experimento, **TODOS** los puntos deben cumplirse:

### ğŸ§ª Entorno

* [ ] El repo fue actualizado (`git pull`)
* [ ] El entorno se creÃ³ con `uv sync --locked`
* [ ] No se instalaron paquetes manualmente con `pip install`, sino con `uv add`

### ğŸ§  ConfiguraciÃ³n del experimento (`train.py`)

* [ ] `RUN_NAME` es descriptivo y Ãºnico (incluye idea + fecha)
* [ ] `DS_NAME` corresponde al autor real del experimento
* [ ] `STAGE` es correcto (`training` salvo indicaciÃ³n del MLE)

### ğŸ“¦ Dataset

* [ ] El dataset usado estÃ¡ en `data/clean/`
* [ ] El nombre del archivo es correcto
* [ ] El dataset fue logueado con `log_dataset_metadata`
* [ ] La versiÃ³n del dataset fue incrementada si hubo cambios

### ğŸ§  Feature Engineering (`features.py`)

* [ ] No se cambiÃ³ la firma ni el return de la funciÃ³n
* [ ] El split es reproducible (usa `random_state`)

### ğŸ“Š MÃ©tricas (`metrics.py`)

* [ ] Los nombres de mÃ©tricas son claros y estables
* [ ] Todas las mÃ©tricas retornadas se registran en MLflow

### ğŸ¤– Modelo (`model.py` + `train.py`)

* [ ] El algoritmo estÃ¡ claramente definido
* [ ] Los hiperparÃ¡metros son explÃ­citos
* [ ] `model_family` refleja correctamente el tipo de modelo
* [ ] Se usa el logger de MLflow correcto para el framework

### ğŸ” Reproducibilidad

* [ ] `random_state` estÃ¡ logueado
* [ ] VersiÃ³n de Python y OS estÃ¡n logueadas
* [ ] El experimento puede re-ejecutarse sin cambios manuales

### ğŸ” DagsHub / MLflow

* [ ] Sos colaborador del repositorio en DagsHub
* [ ] El run aparece correctamente en la UI de MLflow
* [ ] Se registraron parÃ¡metros, mÃ©tricas, artifacts y modelo

---

ğŸ“Œ **Si algÃºn punto falla, el experimento NO se considera vÃ¡lido.**
