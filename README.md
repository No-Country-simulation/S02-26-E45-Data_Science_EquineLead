![banner](./assets/equinelead_logo_github.jpg)

# EquineLead: Data-Driven Growth Engine for the Horse Industry

![Python](https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white)
![Docker](https://img.shields.io/badge/Docker-2496ED?style=for-the-badge&logo=docker&logoColor=white)
![Prefect](https://img.shields.io/badge/Prefect-ffffff?style=for-the-badge&logo=prefect&logoColor=070E10)
![UV](https://img.shields.io/badge/UV-000000?style=for-the-badge&logo=astral&logoColor=white)
![Google Cloud](https://img.shields.io/badge/Google_Cloud-4285F4?style=for-the-badge&logo=google-cloud&logoColor=white)
![Playwright](https://img.shields.io/badge/Playwright-2EAD33?style=for-the-badge&logo=playwright&logoColor=white)
![BeautifulSoup](https://img.shields.io/badge/BeautifulSoup-4E9A06?style=for-the-badge&logo=python&logoColor=white)
![LXML](https://img.shields.io/badge/LXML-A90533?style=for-the-badge&logo=xml&logoColor=white)
![Pandas](https://img.shields.io/badge/Pandas-150458?style=for-the-badge&logo=pandas&logoColor=white)
![NumPy](https://img.shields.io/badge/NumPy-013243?style=for-the-badge&logo=numpy&logoColor=white)
![Apache Parquet](https://img.shields.io/badge/Apache_Parquet-6070AD?style=for-the-badge&logo=apache&logoColor=white)
![Faker](https://img.shields.io/badge/Faker-CF4141?style=for-the-badge&logo=python&logoColor=white)

**EquineLead** es un motor de crecimiento basado en datos dise√±ado para resolver la fragmentaci√≥n del mercado ecuestre. Este sistema transforma la navegaci√≥n casual en leads calificados mediante la integraci√≥n de scrapers inteligentes, embudos automatizados y modelos de propensi√≥n de compra.

---

## üìñ Tabla de Contenidos
- [Definici√≥n del Problema](#definici√≥n-del-problema)
- [Arquitectura y Stack](#arquitectura-del-sistema)
- [Infraestructura (Terraform)](#-infraestructura-como-c√≥digo-iac)
- [Pipeline de Datos](#pipeline-de-datos-etlelt)
- [Gu√≠a de Ejecuci√≥n R√°pida](#gu√≠a-de-ejecuci√≥n-quick-start)

---

## Definici√≥n del Problema (Business Understanding)
### El Desaf√≠o
La industria ecuestre opera en un ecosistema nicho, altamente fragmentado y con costos de adquisici√≥n (CAC) elevados. Actualmente, identificar a un comprador de un caballo de salto de $50,000 frente a un entusiasta casual es una tarea manual e ineficiente.

### Objetivos del Proyecto
+ **Identificaci√≥n de Leads de Alto Valor**: Clasificar autom√°ticamente usuarios en los cuatro verticales: Eventos, Servicios, Caballos y Equipamiento.
+ **Reducci√≥n del Ciclo de Venta**: Acortar el tiempo entre el "inter√©s inicial" y la "calificaci√≥n (SQL)" mediante scoring predictivo.
+ **Optimizaci√≥n de B2B y B2C**: Diferenciar el comportamiento de propietarios individuales frente a administradores de centros h√≠picos o mayoristas.

### KPIs de √âxito
+ **Lead Quality Score (LQS)**: Precisi√≥n del modelo para predecir la conversi√≥n (Meta: >80%).
+ **CAC Reduction**: Reducci√≥n esperada del 15% en costos de marketing mediante segmentaci√≥n precisa.
+ **Conversion Rate (CVR)**: Mejora del flujo de ventas en el vertical de "Caballos de Alto Valor".

---

## Arquitectura del Sistema
El proyecto est√° dise√±ado bajo principios de Modern Data Stack, priorizando la velocidad de ejecuci√≥n y la observabilidad.

### üõ† Stack Tecnol√≥gico

+ **Orquestaci√≥n**: Prefect (Local + Prefect Cloud).
+ **Gesti√≥n de Entorno**: UV (Instalaci√≥n de dependencias 70% m√°s r√°pida que pip).
+ **Contenerizaci√≥n**: Docker & Docker-compose.
+ **Ingesta**: Playwright (Din√°mico), BeautifulSoup4 (Est√°tico)
+ **Cloud Storage**: (Data Lake en formato Parquet)
+ **Data Synthesis**: Python Faker + Proyecciones de [Rees46 Dataset](https://www.kaggle.com/mkechinov/ecommerce-behavior-data-from-multi-category-store).

---

## üèóÔ∏è Infraestructura como C√≥digo (IaC)

Para garantizar la reproducibilidad total, la infraestructura de la nube (Google Cloud Storage) se gestiona mediante **Terraform**. Esto permite levantar el Data Lake y configurar los permisos necesarios en segundos.

### Configuraci√≥n de Infraestructura

1.  **Requisitos**: Tener instalado [Terraform](https://www.terraform.io/downloads) y el [Google Cloud CLI](https://cloud.google.com/sdk/docs/install).
2.  **Autenticaci√≥n**:
    ```powershell
    gcloud auth application-default login
    ```
3.  **Personalizaci√≥n**:
    Crea un archivo `infra/terraform/terraform.tfvars` para definir tus variables:
    ```hcl
    project_id           = "tu-id-de-proyecto"
    region               = "us_weast1"
    bucket_name          = "equinelead-datalake"
    storage_class        = "STANDARD"
    service_account_name = "your-admin"
    ```
4.  **Despliegue**:
    ```powershell
    # Inicializar y aplicar cambios
    terraform -chdir=infra/terraform init
    terraform -chdir=infra/terraform validate
    terraform -chdir=infra/terraform plan -out=tfplan
    terraform -chdir=infra/terraform apply "tfplan"
    ```

### Gesti√≥n de Credenciales
Una vez completado el `apply`, Terraform generar√° una Service Account Key. Extr√°ela para que el pipeline de Docker pueda autenticarse:
```powershell
$rawKey = terraform -chdir=infra/terraform output -raw service_account_key
[System.Text.Encoding]::UTF8.GetString([System.Convert]::FromBase64String($rawKey)) | Out-File -FilePath "./secrets/gcp-sa-key.json" -Encoding ascii

---

## Pipeline de Datos (ETL/ELT)

![pipeline_run](./assets/demo_flow_data_pipeline.png)

### Ingesta y Scraping Paralelizado
El pipeline ejecuta m√∫ltiples scrapers de forma concurrente dentro de contenedores Docker:
+ **Playwright**: Para la extracci√≥n de datos en sitios din√°micos de subastas y clasificados.
+ **BeautifulSoup4 + lxml**: Para el procesamiento r√°pido de directorios est√°ticos de servicios y eventos.

### Generaci√≥n de Datos Sint√©ticos (Behavioral Tracking)
Para simular el comportamiento de usuario, se mapearon los eventos del dataset de Rees46 a un entorno ecuestre ficticio:

+ **Mapeo de Categor√≠as**: Los productos electr√≥nicos/hogar se transformaron en categor√≠as como Sillas de Salto, Suplementos y Publicaciones de Caballos.
+ **Identidades con Faker**: Se generaron perfiles de usuarios √∫nicos (Leads) con historiales de navegaci√≥n coherentes.
+ **Proyecci√≥n de Eventos**: Se recrearon funnels de conversi√≥n (view -> cart -> purchase) para identificar patrones de "Intenci√≥n de Compra".

### Limpieza y Carga (GCP)
+ **Transformaci√≥n**: Limpieza de strings, normalizaci√≥n de datos numericos y manejo de valores nulos en paralelo.
+ **Storage**: Los datos finales se serializan en Parquet para optimizar el peso y la velocidad de consulta, y se suben a un bucket de Google Cloud Storage.

### Diagrama Entidad-Relaci√≥n (DER)

![DER](./assets/equinelead.svg)

---

## Gu√≠a de Ejecuci√≥n (Quick Start)
Este proyecto es totalmente reproducible y "Plug & Play".

> Requisitos: Docker y una cuenta en Prefect Cloud (opcional para logs).

#### Clonar el repositorio:

```bash
git clone https://github.com/No-Country-simulation/S02-26-E45-Data_Science_EquineLead
cd S02-26-E45-Data_Science_EquineLead
```
#### Configurar variables de entorno:
Crea un archivo .env con tus credenciales de GCP y el API Key de Prefect.

```bash
PREFECT_API_URL="https://api.prefect.cloud/api/accounts/[ACCOUNT-ID]/workspaces/[WORKSPACE-ID]"
PREFECT_API_KEY="[API-KEY]"
GCP_PROJECT_ID="tu_id_proyecto"
GCP_BUCKET_NAME="tu_nombre_bucket"
GOOGLE_APPLICATION_CREDENTIALS="path_to_credentials_json"
```

#### Loguearte en Prefect Cloud:

```bash
prefect cloud login
```

#### Levantar la infraestructura:

```bash
docker compose --profile pipeline up --build
```

Este comando levantar√° el agente de Prefect, construira el contenedor e instalar√° dependencias con UV y disparar√° el flujo de ingesta.

> *Nota T√©cnica*: Gracias al gestor UV, la construcci√≥n de la imagen ignora el overhead de pip, logrando entornos listos en segundos