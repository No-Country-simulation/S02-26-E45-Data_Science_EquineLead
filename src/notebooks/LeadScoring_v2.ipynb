{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "v2_md_39276",
   "metadata": {},
   "source": [
    "# Feature Engineering + Threshold Tuning + LightGBM + CatBoost\n",
    "\n",
    "> El objetivo de este notebook es mejorar el F2 Lead Oro sobre el campeón actual (`XGB_Baseline`, F2=0.4538) entrenado en el notebook ***LeadScoring***.\n",
    "\n",
    "Todos los runs se logean en el mismo experimento `EquineLead_LeadScoring` de DagsHub  \n",
    "con prefijo `v2_*` para distinguirlos de la primera iteración."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "v2_md_02025",
   "metadata": {},
   "source": [
    "## 0 · Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "580b64fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lightgbm: ✅ OK\n",
      "catboost: ✅ OK\n",
      "optuna: ✅ OK\n"
     ]
    }
   ],
   "source": [
    "import subprocess, sys\n",
    "\n",
    "paquetes = ['lightgbm', 'catboost', 'optuna']\n",
    "for pkg in paquetes:\n",
    "    result = subprocess.run(\n",
    "        ['uv', 'pip', 'install', pkg, '--python', sys.executable],\n",
    "        capture_output=True, text=True\n",
    "    )\n",
    "    print(f'{pkg}: {\"✅\" if result.returncode == 0 else \"❌\"} {result.stderr[:150] if result.returncode != 0 else \"OK\"}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "v2_cd_35782",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mlflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmlflow\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgetpass\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'mlflow'"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ['MLFLOW_TRACKING_USERNAME'] = 'ITRoselloSignoris'\n",
    "os.environ['MLFLOW_TRACKING_PASSWORD'] = getpass.getpass('DagsHub token: ')\n",
    "\n",
    "mlflow.set_tracking_uri('https://dagshub.com/aletbm/S02-26-E45-Data_Science_EquineLead.mlflow')\n",
    "\n",
    "EXPERIMENT_NAME = 'EquineLead_LeadScoring'\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "v2_cd_71353",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'optuna'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwarnings\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtempfile\u001b[39;00m,\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m,\u001b[38;5;250m \u001b[39m\u001b[34;01mpickle\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01moptuna\u001b[39;00m\n\u001b[32m      7\u001b[39m optuna.logging.set_verbosity(optuna.logging.WARNING)\n\u001b[32m      8\u001b[39m warnings.filterwarnings(\u001b[33m'\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'optuna'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import tempfile, os, pickle\n",
    "import optuna\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import fbeta_score, precision_recall_curve, make_scorer\n",
    "from sklearn.preprocessing import TargetEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "f2_scorer   = make_scorer(fbeta_score, beta=2)\n",
    "cv5         = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "orden_leads = ['Lead Bronce', 'Lead Plata', 'Lead Oro']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "v2_md_24725",
   "metadata": {},
   "source": [
    "## 1 · Cargar el dataset y Feature Engineering\n",
    "\n",
    "Cargamos `df_final.parquet` que tiene **todas** las columnas originales incluidas las categóricas  \n",
    "que se dropearon. Acá las recuperamos y construimos features nuevas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "v2_cd_16458",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('../../data/clean/df_final.parquet')\n",
    "print(f'Shape: {df.shape}')\n",
    "print(f'Columnas: {list(df.columns)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "v2_md_36935",
   "metadata": {},
   "source": [
    "### 1.1 · Features de ratios e intensidad\n",
    "\n",
    "Estas features capturan **intención real** mejor que los conteos absolutos.  \n",
    "Un usuario que vio 3 caballos y metió 2 al carrito es muy diferente a uno que vio 50 y metió 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "v2_cd_90414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# — Ratios de engagement (intención / exposición) —\n",
    "df['ratio_cart_horse']   = df['horses_added_to_cart']  / (df['horses_viewed']  + 1)\n",
    "df['ratio_cart_prods']   = df['products_added_to_cart'] / (df['products_viewed'] + 1)\n",
    "df['ratio_cart_global']  = df['total_cart_adds'] / (df['total_views'] + 1)\n",
    "\n",
    "# — Precio aspiracional (¿apunta por encima del promedio?) —\n",
    "df['precio_aspiracional_horse'] = df['max_horse_price_viewed'] / (df['avg_horse_price_viewed'] + 1)\n",
    "df['precio_aspiracional_prods'] = df['max_product_price_viewed'] / (df['avg_product_price_viewed'] + 1)\n",
    "\n",
    "# — Rango de precio (foco vs exploración) —\n",
    "df['rango_precio_horse'] = df['max_horse_price_viewed'] - df['min_horse_price_viewed']\n",
    "\n",
    "# — Brecha de prestige (¿más exigente en caballos que en productos?) —\n",
    "df['prestige_gap'] = df['avg_prestige_score_horses'] - df['avg_prestige_score_products']\n",
    "\n",
    "# — Proporción de vistas de caballo sobre total —\n",
    "df['ratio_horse_views'] = df['horses_viewed'] / (df['total_views'] + 1)\n",
    "\n",
    "nuevas = ['ratio_cart_horse', 'ratio_cart_prods', 'ratio_cart_global',\n",
    "          'precio_aspiracional_horse', 'precio_aspiracional_prods',\n",
    "          'rango_precio_horse', 'prestige_gap', 'ratio_horse_views']\n",
    "print(f'Features nuevas: {nuevas}')\n",
    "print(df[nuevas].describe().round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "v2_md_27165",
   "metadata": {},
   "source": [
    "### 1.2 · Encoding de categóricas\n",
    "\n",
    "Las 6 columnas categóricas se encodean con **Target Encoding** calculado sobre train  \n",
    "y aplicado a test — sin leakage.  \n",
    "\n",
    "Para CatBoost las dejamos como strings porque CatBoost las maneja nativamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "v2_cd_95010",
   "metadata": {},
   "outputs": [],
   "source": [
    "CAT_COLS = [\n",
    "    'gender_with_most_appearances',\n",
    "    'breedFamily_with_most_appearances',\n",
    "    'color_grouped_with_most_appearances',\n",
    "    'most_viewed_category',\n",
    "    'most_viewed_brand',\n",
    "    'most_viewed_target_user',\n",
    "]\n",
    "\n",
    "# Verificamos que existen en el parquet\n",
    "disponibles = [c for c in CAT_COLS if c in df.columns]\n",
    "faltantes   = [c for c in CAT_COLS if c not in df.columns]\n",
    "print(f'Disponibles: {disponibles}')\n",
    "print(f'Faltantes:   {faltantes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "v2_cd_95711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columnas a dropear (redundantes o sin señal)\n",
    "DROP_COLS = [\n",
    "    'has_both_interests',  # redundante con has_registry_viewed + has_shipping_viewed\n",
    "    'total_views',         # correlación > 0.86 con horses_viewed + products_viewed\n",
    "    'avg_height',          # característica del caballo, no del comportamiento del usuario\n",
    "    'avg_weight',          # idem\n",
    "]\n",
    "DROP_COLS = [c for c in DROP_COLS if c in df.columns]\n",
    "\n",
    "df_model = df.drop(columns=DROP_COLS)\n",
    "print(f'Shape después de drops: {df_model.shape}')\n",
    "print(f'Columnas dropeadas: {DROP_COLS}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "v2_cd_78375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split estratificado\n",
    "X = df_model.drop(columns=['horse_target', 'prods_target'])\n",
    "y = df_model[['horse_target', 'prods_target']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, stratify=y, test_size=0.2, random_state=42\n",
    ")\n",
    "print(f'X_train: {X_train.shape} | X_test: {X_test.shape}')\n",
    "\n",
    "# Target Encoding sobre train, aplicado a test (sin leakage)\n",
    "# Usamos horse_target como target de referencia para el encoding\n",
    "y_train_enc = y_train['horse_target'].map({'Lead Bronce': 0, 'Lead Plata': 1, 'Lead Oro': 2})\n",
    "\n",
    "te = TargetEncoder(target_type='continuous', random_state=42)\n",
    "X_train[disponibles] = te.fit_transform(X_train[disponibles], y_train_enc)\n",
    "X_test[disponibles]  = te.transform(X_test[disponibles])\n",
    "\n",
    "print(f'Target encoding aplicado a: {disponibles}')\n",
    "print(f'Features totales: {X_train.shape[1]}')\n",
    "print(f'Features: {list(X_train.columns)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "v2_md_79727",
   "metadata": {},
   "source": [
    "### 1.3 · Targets binarios y balanceo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "v2_cd_56272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 1: Bronce (0) vs Plata/Oro (1)\n",
    "y_train_p1_horse = (y_train['horse_target'] != 'Lead Bronce').astype(int)\n",
    "y_train_p1_prods = (y_train['prods_target'] != 'Lead Bronce').astype(int)\n",
    "y_test_p1_horse  = (y_test['horse_target']  != 'Lead Bronce').astype(int)\n",
    "y_test_p1_prods  = (y_test['prods_target']  != 'Lead Bronce').astype(int)\n",
    "\n",
    "smote_p1 = SMOTE(random_state=42)\n",
    "X_p1h_bal, y_p1h_bal = smote_p1.fit_resample(X_train, y_train_p1_horse)\n",
    "X_p1p_bal, y_p1p_bal = smote_p1.fit_resample(X_train, y_train_p1_prods)\n",
    "\n",
    "# Paso 2: Plata (0) vs Oro (1)\n",
    "mask_p2_horse = y_train['horse_target'] != 'Lead Bronce'\n",
    "mask_p2_prods = y_train['prods_target'] != 'Lead Bronce'\n",
    "X_p2h_raw = X_train[mask_p2_horse]\n",
    "X_p2p_raw = X_train[mask_p2_prods]\n",
    "y_p2h_raw = (y_train['horse_target'][mask_p2_horse] == 'Lead Oro').astype(int)\n",
    "y_p2p_raw = (y_train['prods_target'][mask_p2_prods] == 'Lead Oro').astype(int)\n",
    "\n",
    "smote_p2 = SMOTE(random_state=42)\n",
    "X_p2h_bal, y_p2h_bal = smote_p2.fit_resample(X_p2h_raw, y_p2h_raw)\n",
    "X_p2p_bal, y_p2p_bal = smote_p2.fit_resample(X_p2p_raw, y_p2p_raw)\n",
    "\n",
    "mask_test_p2_horse = y_test['horse_target'] != 'Lead Bronce'\n",
    "mask_test_p2_prods = y_test['prods_target'] != 'Lead Bronce'\n",
    "y_test_p2_horse = (y_test['horse_target'][mask_test_p2_horse] == 'Lead Oro').astype(int)\n",
    "y_test_p2_prods = (y_test['prods_target'][mask_test_p2_prods] == 'Lead Oro').astype(int)\n",
    "\n",
    "spw_p1h = (y_train_p1_horse==0).sum() / (y_train_p1_horse==1).sum()\n",
    "spw_p1p = (y_train_p1_prods==0).sum() / (y_train_p1_prods==1).sum()\n",
    "spw_p2h = (y_p2h_raw==0).sum() / (y_p2h_raw==1).sum()\n",
    "spw_p2p = (y_p2p_raw==0).sum() / (y_p2p_raw==1).sum()\n",
    "\n",
    "print(f'P1 bal: {y_p1h_bal.value_counts().to_dict()}')\n",
    "print(f'P2 bal: {y_p2h_bal.value_counts().to_dict()}')\n",
    "print(f'SPW p2 horse: {spw_p2h:.2f} | SPW p2 prods: {spw_p2p:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "v2_md_14470",
   "metadata": {},
   "source": [
    "## 2 · Funciones de evaluación y logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "v2_cd_16303",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predecir_cascada(X, m1, m2):\n",
    "    pred = np.array(['Lead Bronce'] * len(X), dtype=object)\n",
    "    mask = m1.predict(X) == 1\n",
    "    if mask.sum() > 0:\n",
    "        pred[mask] = np.where(m2.predict(X[mask]) == 1, 'Lead Oro', 'Lead Plata')\n",
    "    return pred\n",
    "\n",
    "def predecir_cascada_proba(X, m1, m2):\n",
    "    \"\"\"Devuelve probabilidad de ser Oro para threshold tuning.\"\"\"\n",
    "    proba_p1 = m1.predict_proba(X)[:, 1]\n",
    "    proba_p2 = m2.predict_proba(X)[:, 1]\n",
    "    # Probabilidad de Oro = P(pasa P1) * P(es Oro dado que pasó P1)\n",
    "    return proba_p1 * proba_p2\n",
    "\n",
    "def calcular_metricas(p1h, p1p, p2h, p2p):\n",
    "    metricas = {}\n",
    "    for target, m1, m2, y_true, X_te_p2, y_te_p2 in [\n",
    "        ('horse', p1h, p2h, y_test['horse_target'],\n",
    "         X_test[mask_test_p2_horse], y_test_p2_horse),\n",
    "        ('prods', p1p, p2p, y_test['prods_target'],\n",
    "         X_test[mask_test_p2_prods], y_test_p2_prods),\n",
    "    ]:\n",
    "        y_pred = predecir_cascada(X_test, m1, m2)\n",
    "        f2_macro = fbeta_score(y_true, y_pred, beta=2, average='macro', labels=orden_leads)\n",
    "        f2_oro   = fbeta_score(y_true, y_pred, beta=2, labels=['Lead Oro'], average='macro')\n",
    "        f2_p2_tr = fbeta_score(\n",
    "            y_p2h_raw if target=='horse' else y_p2p_raw,\n",
    "            m2.predict(X_p2h_raw if target=='horse' else X_p2p_raw), beta=2\n",
    "        )\n",
    "        f2_p2_te = fbeta_score(y_te_p2, m2.predict(X_te_p2), beta=2)\n",
    "        metricas[target] = {\n",
    "            'f2_macro': f2_macro, 'f2_lead_oro': f2_oro,\n",
    "            'f2_paso2_train': f2_p2_tr, 'f2_paso2_test': f2_p2_te,\n",
    "            'overfit_gap_p2': f2_p2_tr - f2_p2_te,\n",
    "            'y_pred': y_pred,\n",
    "        }\n",
    "    return metricas\n",
    "\n",
    "def loguear_run(run_name, model_type, params_p1, params_p2,\n",
    "                p1h, p1p, p2h, p2p, metricas, extra_metrics=None):\n",
    "    with mlflow.start_run(run_name=run_name):\n",
    "        mlflow.set_tag('model_type', model_type)\n",
    "        mlflow.set_tag('version', 'v2')\n",
    "        for k, v in params_p1.items(): mlflow.log_param(f'p1_{k}', v)\n",
    "        for k, v in params_p2.items(): mlflow.log_param(f'p2_{k}', v)\n",
    "        for target, m in metricas.items():\n",
    "            for metric, value in m.items():\n",
    "                if metric != 'y_pred':\n",
    "                    mlflow.log_metric(f'{metric}_{target}', value)\n",
    "        if extra_metrics:\n",
    "            for k, v in extra_metrics.items(): mlflow.log_metric(k, v)\n",
    "        # Guardamos modelos como pickle\n",
    "        for nombre, modelo in [('p1_horse',p1h),('p1_prods',p1p),('p2_horse',p2h),('p2_prods',p2p)]:\n",
    "            with tempfile.NamedTemporaryFile(suffix='.pkl', delete=False) as tmp:\n",
    "                pickle.dump(modelo, tmp)\n",
    "                mlflow.log_artifact(tmp.name, nombre)\n",
    "                os.unlink(tmp.name)\n",
    "        run_id = mlflow.active_run().info.run_id\n",
    "        print(f'\\nRun logueado: {run_name} | run_id: {run_id}')\n",
    "    return run_id\n",
    "\n",
    "def check_overfitting(configs, nombre):\n",
    "    print(f'\\n  Chequeo Overfitting — {nombre}')\n",
    "    print(f'{\"─\"*60}')\n",
    "    print(f'{\"Paso\":<18} {\"F2 Train\":>10} {\"F2 Test\":>10} {\"Gap\":>8}  Status')\n",
    "    print('─'*60)\n",
    "    for label, model, X_tr, y_tr, X_te, y_te in configs:\n",
    "        f2_tr = fbeta_score(y_tr, model.predict(X_tr), beta=2)\n",
    "        f2_te = fbeta_score(y_te, model.predict(X_te), beta=2)\n",
    "        gap   = f2_tr - f2_te\n",
    "        status = 'overfit' if gap > 0.10 else 'OK'\n",
    "        print(f'{label:<18} {f2_tr:>10.4f} {f2_te:>10.4f} {gap:>8.4f}  {status}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "v2_md_88253",
   "metadata": {},
   "source": [
    "## 3 · Threshold Tuning sobre el campeón\n",
    "\n",
    "El XGB_Baseline usa threshold=0.50 por defecto.  \n",
    "Con 620 Lead Oro vs 11343 Lead Plata, muchos Oros reales tienen probabilidad 0.30-0.49  \n",
    "y quedan clasificados como Plata. Buscamos el umbral que maximiza F2 Lead Oro en test.\n",
    "\n",
    "Cómo F2 penaliza los falsos negativos 2x más que los falsos positivos. Bajar el umbral  \n",
    "aumenta el recall (recuperamos más Oros reales) a costa de algo de precision  \n",
    "(algunos Plata se clasifican como Oro).   \n",
    "  \n",
    "Dado el peso de la métrica, ese tradeoff conviene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "v2_cd_86416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos el campeón de DagsHub\n",
    "# run_id del XGB_Baseline \n",
    "RUN_ID_CAMPEON_V1 = '74f1707c89964d69866d337f07624075'  # ← XGB_Baseline v1\n",
    "\n",
    "xgb_champ_p1h = mlflow.sklearn.load_model(f'runs:/{RUN_ID_CAMPEON_V1}/p1_horse') \\\n",
    "    if False else pickle.load(open(mlflow.artifacts.download_artifacts(\n",
    "        f'runs:/{RUN_ID_CAMPEON_V1}/p1_horse', dst_path='/tmp/champ'\n",
    "    ) + '/tmp_file', 'rb')) if False else None\n",
    "\n",
    "# Alternativa más simple: reentrenar el campeón con los nuevos features\n",
    "# (los nuevos features hacen que el modelo cargado no sea directamente comparable)\n",
    "print(' El campeón de v1 fue entrenado con 29 features.')\n",
    "print(f'   Este dataset tiene {X_train.shape[1]} features (incluye nuevas + categóricas).')\n",
    "print('   Threshold tuning se hace sobre modelos reentrenados con el nuevo feature set.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "v2_cd_42898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reentrenamos XGB con los mejores params de v1 sobre el nuevo feature set\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb_params_base = dict(\n",
    "    n_estimators=300, max_depth=4, learning_rate=0.05,\n",
    "    subsample=0.8, colsample_bytree=0.8,\n",
    "    eval_metric='aucpr', tree_method='hist',\n",
    "    random_state=42, n_jobs=-1\n",
    ")\n",
    "\n",
    "xgb_ref_p1h = XGBClassifier(**xgb_params_base, scale_pos_weight=spw_p1h).fit(X_train, y_train_p1_horse)\n",
    "xgb_ref_p1p = XGBClassifier(**xgb_params_base, scale_pos_weight=spw_p1p).fit(X_train, y_train_p1_prods)\n",
    "xgb_ref_p2h = XGBClassifier(**xgb_params_base, scale_pos_weight=spw_p2h).fit(X_p2h_raw, y_p2h_raw)\n",
    "xgb_ref_p2p = XGBClassifier(**xgb_params_base, scale_pos_weight=spw_p2p).fit(X_p2p_raw, y_p2p_raw)\n",
    "\n",
    "print('Modelos de referencia entrenados con nuevo feature set.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "v2_cd_76107",
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold_tuning(m1, m2, X_te, y_true, target_name, thresholds=None):\n",
    "    \"\"\"Busca el umbral que maximiza F2 Lead Oro.\"\"\"\n",
    "    if thresholds is None:\n",
    "        thresholds = np.arange(0.10, 0.70, 0.02)\n",
    "\n",
    "    proba_oro = predecir_cascada_proba(X_te, m1, m2)\n",
    "    resultados = []\n",
    "    for t in thresholds:\n",
    "        # Aplicamos la cascada con threshold\n",
    "        pred = np.array(['Lead Bronce'] * len(X_te), dtype=object)\n",
    "        mask_p1 = m1.predict_proba(X_te)[:, 1] >= 0.5 \n",
    "        if mask_p1.sum() > 0:\n",
    "            proba_p2 = m2.predict_proba(X_te[mask_p1])[:, 1]\n",
    "            pred[mask_p1] = np.where(proba_p2 >= t, 'Lead Oro', 'Lead Plata')\n",
    "        f2_oro   = fbeta_score(y_true, pred, beta=2, labels=['Lead Oro'], average='macro')\n",
    "        f2_macro = fbeta_score(y_true, pred, beta=2, average='macro', labels=orden_leads)\n",
    "        resultados.append({'threshold': t, 'f2_oro': f2_oro, 'f2_macro': f2_macro})\n",
    "\n",
    "    df_res = pd.DataFrame(resultados)\n",
    "    best = df_res.loc[df_res['f2_oro'].idxmax()]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 4))\n",
    "    ax.plot(df_res['threshold'], df_res['f2_oro'],   label='F2 Lead Oro', color='#C44E52', lw=2)\n",
    "    ax.plot(df_res['threshold'], df_res['f2_macro'], label='F2 macro',    color='#4C72B0', lw=2, ls='--')\n",
    "    ax.axvline(best['threshold'], color='gray', ls=':', label=f'Óptimo: {best[\"threshold\"]:.2f}')\n",
    "    ax.set_xlabel('Threshold P2'); ax.set_ylabel('F2')\n",
    "    ax.set_title(f'Threshold Tuning — {target_name}')\n",
    "    ax.legend(); ax.grid(alpha=0.3)\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "    print(f'{target_name}: threshold óptimo = {best[\"threshold\"]:.2f} | '\n",
    "          f'F2 Oro = {best[\"f2_oro\"]:.4f} | F2 macro = {best[\"f2_macro\"]:.4f}')\n",
    "    return float(best['threshold']), df_res\n",
    "\n",
    "print('── Threshold tuning — horse ──')\n",
    "thresh_horse, df_thresh_h = threshold_tuning(\n",
    "    xgb_ref_p1h, xgb_ref_p2h, X_test, y_test['horse_target'], 'horse'\n",
    ")\n",
    "print('\\n── Threshold tuning — prods ──')\n",
    "thresh_prods, df_thresh_p = threshold_tuning(\n",
    "    xgb_ref_p1p, xgb_ref_p2p, X_test, y_test['prods_target'], 'prods'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "v2_cd_40164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluamos con el threshold óptimo\n",
    "def calcular_metricas_con_threshold(p1h, p1p, p2h, p2p, th_horse, th_prods):\n",
    "    metricas = {}\n",
    "    for target, m1, m2, y_true, th, X_te_p2, y_te_p2 in [\n",
    "        ('horse', p1h, p2h, y_test['horse_target'], th_horse,\n",
    "         X_test[mask_test_p2_horse], y_test_p2_horse),\n",
    "        ('prods', p1p, p2p, y_test['prods_target'], th_prods,\n",
    "         X_test[mask_test_p2_prods], y_test_p2_prods),\n",
    "    ]:\n",
    "        pred = np.array(['Lead Bronce'] * len(X_test), dtype=object)\n",
    "        mask_p1 = m1.predict_proba(X_test)[:, 1] >= 0.5\n",
    "        if mask_p1.sum() > 0:\n",
    "            proba_p2 = m2.predict_proba(X_test[mask_p1])[:, 1]\n",
    "            pred[mask_p1] = np.where(proba_p2 >= th, 'Lead Oro', 'Lead Plata')\n",
    "        f2_macro = fbeta_score(y_true, pred, beta=2, average='macro', labels=orden_leads)\n",
    "        f2_oro   = fbeta_score(y_true, pred, beta=2, labels=['Lead Oro'], average='macro')\n",
    "        f2_p2_tr = fbeta_score(\n",
    "            y_p2h_raw if target=='horse' else y_p2p_raw,\n",
    "            m2.predict(X_p2h_raw if target=='horse' else X_p2p_raw), beta=2\n",
    "        )\n",
    "        f2_p2_te = fbeta_score(y_te_p2, m2.predict(X_te_p2), beta=2)\n",
    "        metricas[target] = {\n",
    "            'f2_macro': f2_macro, 'f2_lead_oro': f2_oro,\n",
    "            'f2_paso2_train': f2_p2_tr, 'f2_paso2_test': f2_p2_te,\n",
    "            'overfit_gap_p2': f2_p2_tr - f2_p2_te,\n",
    "            'threshold': th, 'y_pred': pred,\n",
    "        }\n",
    "    return metricas\n",
    "\n",
    "metricas_xgb_thresh = calcular_metricas_con_threshold(\n",
    "    xgb_ref_p1h, xgb_ref_p1p, xgb_ref_p2h, xgb_ref_p2p, thresh_horse, thresh_prods\n",
    ")\n",
    "for t, m in metricas_xgb_thresh.items():\n",
    "    print(f'{t}: F2 macro={m[\"f2_macro\"]:.4f} | F2 Oro={m[\"f2_lead_oro\"]:.4f} '\n",
    "          f'| Gap={m[\"overfit_gap_p2\"]:.4f} | threshold={m[\"threshold\"]:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "v2_cd_77301",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id_xgb_thresh = loguear_run(\n",
    "    run_name   = 'v2_XGB_ThresholdTuning',\n",
    "    model_type = 'XGBoost',\n",
    "    params_p1  = {**xgb_params_base, 'scale_pos_weight': spw_p1h},\n",
    "    params_p2  = {**xgb_params_base, 'scale_pos_weight': spw_p2h,\n",
    "                  'threshold_horse': thresh_horse, 'threshold_prods': thresh_prods},\n",
    "    p1h=xgb_ref_p1h, p1p=xgb_ref_p1p,\n",
    "    p2h=xgb_ref_p2h, p2p=xgb_ref_p2p,\n",
    "    metricas=metricas_xgb_thresh\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "v2_md_89903",
   "metadata": {},
   "source": [
    "## 4 · LightGBM con Optuna\n",
    "\n",
    "LightGBM es más rápido que XGBoost en CPU gracias a su histogram-based splitting  \n",
    "leaf-wise (vs level-wise de XGBoost). Esto permite explorar un espacio de  \n",
    "hiperparámetros más grande en el mismo tiempo.  \n",
    "\n",
    "Optuna usa optimización bayesiana (TPE sampler) para dirigir la búsqueda hacia  \n",
    "las regiones más prometedoras — mucho más eficiente que RandomizedSearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "v2_cd_60539",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crear_objetivo_lgbm(X_tr, y_tr, spw, use_smote=False):\n",
    "    \"\"\"Factory que devuelve la función objetivo para Optuna.\"\"\"\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            'n_estimators':    trial.suggest_int('n_estimators', 100, 600),\n",
    "            'max_depth':       trial.suggest_int('max_depth', 3, 8),\n",
    "            'learning_rate':   trial.suggest_float('learning_rate', 0.01, 0.2, log=True),\n",
    "            'num_leaves':      trial.suggest_int('num_leaves', 15, 63),\n",
    "            'min_child_samples': trial.suggest_int('min_child_samples', 10, 100),\n",
    "            'subsample':       trial.suggest_float('subsample', 0.6, 1.0),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "            'reg_alpha':       trial.suggest_float('reg_alpha', 0, 2.0),\n",
    "            'reg_lambda':      trial.suggest_float('reg_lambda', 0, 5.0),\n",
    "            'scale_pos_weight': spw,\n",
    "            'random_state': 42, 'n_jobs': -1, 'verbose': -1,\n",
    "        }\n",
    "        if use_smote:\n",
    "            pipe = ImbPipeline([\n",
    "                ('smote', SMOTE(random_state=42)),\n",
    "                ('lgbm', LGBMClassifier(**params))\n",
    "            ])\n",
    "            scores = cross_val_score(pipe, X_tr, y_tr, cv=cv5,\n",
    "                                     scoring=f2_scorer, n_jobs=1)\n",
    "        else:\n",
    "            model = LGBMClassifier(**params)\n",
    "            scores = cross_val_score(model, X_tr, y_tr, cv=cv5,\n",
    "                                     scoring=f2_scorer, n_jobs=1)\n",
    "        return scores.mean()\n",
    "    return objective\n",
    "\n",
    "N_TRIALS = 30  \n",
    "\n",
    "print('── Optuna LightGBM — Paso 1 horse ──')\n",
    "study_p1h = optuna.create_study(direction='maximize')\n",
    "study_p1h.optimize(crear_objetivo_lgbm(X_p1h_bal, y_p1h_bal, spw_p1h), n_trials=N_TRIALS)\n",
    "print(f'Mejor CV F2: {study_p1h.best_value:.4f} | Params: {study_p1h.best_params}')\n",
    "\n",
    "print('\\n── Optuna LightGBM — Paso 1 prods ──')\n",
    "study_p1p = optuna.create_study(direction='maximize')\n",
    "study_p1p.optimize(crear_objetivo_lgbm(X_p1p_bal, y_p1p_bal, spw_p1p), n_trials=N_TRIALS)\n",
    "print(f'Mejor CV F2: {study_p1p.best_value:.4f} | Params: {study_p1p.best_params}')\n",
    "\n",
    "print('\\n── Optuna LightGBM — Paso 2 horse (con SMOTE en CV) ──')\n",
    "study_p2h = optuna.create_study(direction='maximize')\n",
    "study_p2h.optimize(crear_objetivo_lgbm(X_p2h_raw, y_p2h_raw, spw_p2h, use_smote=True), n_trials=N_TRIALS)\n",
    "print(f'Mejor CV F2: {study_p2h.best_value:.4f} | Params: {study_p2h.best_params}')\n",
    "\n",
    "print('\\n── Optuna LightGBM — Paso 2 prods (con SMOTE en CV) ──')\n",
    "study_p2p = optuna.create_study(direction='maximize')\n",
    "study_p2p.optimize(crear_objetivo_lgbm(X_p2p_raw, y_p2p_raw, spw_p2p, use_smote=True), n_trials=N_TRIALS)\n",
    "print(f'Mejor CV F2: {study_p2p.best_value:.4f} | Params: {study_p2p.best_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "v2_cd_84717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamos modelos finales con best params sobre datos completos\n",
    "def entrenar_lgbm_final(study, X_tr, y_tr, spw, use_smote=False):\n",
    "    params = {**study.best_params, 'scale_pos_weight': spw,\n",
    "              'random_state': 42, 'n_jobs': -1, 'verbose': -1}\n",
    "    model = LGBMClassifier(**params)\n",
    "    if use_smote:\n",
    "        X_bal, y_bal = SMOTE(random_state=42).fit_resample(X_tr, y_tr)\n",
    "        model.fit(X_bal, y_bal)\n",
    "    else:\n",
    "        model.fit(X_tr, y_tr)\n",
    "    return model\n",
    "\n",
    "lgbm_p1h = entrenar_lgbm_final(study_p1h, X_p1h_bal, y_p1h_bal, spw_p1h)\n",
    "lgbm_p1p = entrenar_lgbm_final(study_p1p, X_p1p_bal, y_p1p_bal, spw_p1p)\n",
    "lgbm_p2h = entrenar_lgbm_final(study_p2h, X_p2h_raw, y_p2h_raw, spw_p2h, use_smote=True)\n",
    "lgbm_p2p = entrenar_lgbm_final(study_p2p, X_p2p_raw, y_p2p_raw, spw_p2p, use_smote=True)\n",
    "\n",
    "metricas_lgbm = calcular_metricas(lgbm_p1h, lgbm_p1p, lgbm_p2h, lgbm_p2p)\n",
    "for t, m in metricas_lgbm.items():\n",
    "    print(f'{t}: F2 macro={m[\"f2_macro\"]:.4f} | F2 Oro={m[\"f2_lead_oro\"]:.4f} '\n",
    "          f'| Gap={m[\"overfit_gap_p2\"]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "v2_cd_10796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold tuning sobre LightGBM\n",
    "print('── Threshold tuning LGBM — horse ──')\n",
    "thresh_lgbm_horse, _ = threshold_tuning(\n",
    "    lgbm_p1h, lgbm_p2h, X_test, y_test['horse_target'], 'LGBM horse'\n",
    ")\n",
    "print('\\n── Threshold tuning LGBM — prods ──')\n",
    "thresh_lgbm_prods, _ = threshold_tuning(\n",
    "    lgbm_p1p, lgbm_p2p, X_test, y_test['prods_target'], 'LGBM prods'\n",
    ")\n",
    "\n",
    "metricas_lgbm_thresh = calcular_metricas_con_threshold(\n",
    "    lgbm_p1h, lgbm_p1p, lgbm_p2h, lgbm_p2p,\n",
    "    thresh_lgbm_horse, thresh_lgbm_prods\n",
    ")\n",
    "for t, m in metricas_lgbm_thresh.items():\n",
    "    print(f'{t}: F2 macro={m[\"f2_macro\"]:.4f} | F2 Oro={m[\"f2_lead_oro\"]:.4f} '\n",
    "          f'| Gap={m[\"overfit_gap_p2\"]:.4f} | threshold={m[\"threshold\"]:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "v2_cd_79779",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id_lgbm = loguear_run(\n",
    "    run_name   = 'v2_LightGBM_Optuna',\n",
    "    model_type = 'LightGBM',\n",
    "    params_p1  = {**study_p1h.best_params, 'scale_pos_weight': spw_p1h},\n",
    "    params_p2  = {**study_p2h.best_params, 'scale_pos_weight': spw_p2h,\n",
    "                  'threshold_horse': thresh_lgbm_horse, 'threshold_prods': thresh_lgbm_prods},\n",
    "    p1h=lgbm_p1h, p1p=lgbm_p1p,\n",
    "    p2h=lgbm_p2h, p2p=lgbm_p2p,\n",
    "    metricas=metricas_lgbm_thresh,\n",
    "    extra_metrics={'optuna_trials': N_TRIALS}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "v2_md_11810",
   "metadata": {},
   "source": [
    "## 5 · CatBoost con categóricas nativas\n",
    "\n",
    "CatBoost no necesita encodear las categóricas — las procesa internamente con  \n",
    "ordered target statistics, que es más robusto que el Target Encoding manual.  \n",
    "Para aprovechar esto, usamos un dataset separado donde las categóricas son strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "v2_cd_80543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset para CatBoost: categóricas como strings (sin Target Encoding)\n",
    "df_cat = df.drop(columns=DROP_COLS + ['horse_target', 'prods_target'])\n",
    "\n",
    "# Agregar las features de ratios\n",
    "df_cat['ratio_cart_horse']       = df['ratio_cart_horse']\n",
    "df_cat['ratio_cart_prods']       = df['ratio_cart_prods']\n",
    "df_cat['ratio_cart_global']      = df['ratio_cart_global']\n",
    "df_cat['precio_aspiracional_horse'] = df['precio_aspiracional_horse']\n",
    "df_cat['precio_aspiracional_prods'] = df['precio_aspiracional_prods']\n",
    "df_cat['rango_precio_horse']     = df['rango_precio_horse']\n",
    "df_cat['prestige_gap']           = df['prestige_gap']\n",
    "df_cat['ratio_horse_views']      = df['ratio_horse_views']\n",
    "\n",
    "y_cat = df[['horse_target', 'prods_target']]\n",
    "\n",
    "Xc_train, Xc_test, yc_train, yc_test = train_test_split(\n",
    "    df_cat, y_cat, stratify=y_cat, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Índices de columnas categóricas (CatBoost los necesita)\n",
    "cat_features_idx = [list(df_cat.columns).index(c) for c in disponibles if c in df_cat.columns]\n",
    "print(f'Columnas categóricas para CatBoost: {[df_cat.columns[i] for i in cat_features_idx]}')\n",
    "print(f'Índices: {cat_features_idx}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "v2_cd_64219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Targets y balanceo para CatBoost\n",
    "yc_train_p1_horse = (yc_train['horse_target'] != 'Lead Bronce').astype(int)\n",
    "yc_train_p1_prods = (yc_train['prods_target'] != 'Lead Bronce').astype(int)\n",
    "yc_test_p1_horse  = (yc_test['horse_target']  != 'Lead Bronce').astype(int)\n",
    "yc_test_p1_prods  = (yc_test['prods_target']  != 'Lead Bronce').astype(int)\n",
    "\n",
    "# CatBoost no acepta SMOTE directo con categóricas — usamos class_weights\n",
    "mask_p2_cat_horse = yc_train['horse_target'] != 'Lead Bronce'\n",
    "mask_p2_cat_prods = yc_train['prods_target'] != 'Lead Bronce'\n",
    "Xc_p2h = Xc_train[mask_p2_cat_horse]\n",
    "Xc_p2p = Xc_train[mask_p2_cat_prods]\n",
    "yc_p2h = (yc_train['horse_target'][mask_p2_cat_horse] == 'Lead Oro').astype(int)\n",
    "yc_p2p = (yc_train['prods_target'][mask_p2_cat_prods] == 'Lead Oro').astype(int)\n",
    "\n",
    "mask_test_cat_horse = yc_test['horse_target'] != 'Lead Bronce'\n",
    "mask_test_cat_prods = yc_test['prods_target'] != 'Lead Bronce'\n",
    "\n",
    "print('Targets CatBoost listos.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "v2_cd_72901",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crear_objetivo_catboost(X_tr, y_tr, spw, cat_idx):\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            'iterations':      trial.suggest_int('iterations', 100, 500),\n",
    "            'depth':           trial.suggest_int('depth', 3, 8),\n",
    "            'learning_rate':   trial.suggest_float('learning_rate', 0.01, 0.2, log=True),\n",
    "            'l2_leaf_reg':     trial.suggest_float('l2_leaf_reg', 1, 10),\n",
    "            'bagging_temperature': trial.suggest_float('bagging_temperature', 0, 1),\n",
    "            'scale_pos_weight': spw,\n",
    "            'cat_features':    cat_idx,\n",
    "            'random_seed': 42, 'verbose': 0, 'thread_count': -1,\n",
    "            'eval_metric': 'F1',\n",
    "        }\n",
    "        model = CatBoostClassifier(**params)\n",
    "        scores = cross_val_score(model, X_tr, y_tr, cv=cv5,\n",
    "                                 scoring=f2_scorer, n_jobs=1)\n",
    "        return scores.mean()\n",
    "    return objective\n",
    "\n",
    "print('── Optuna CatBoost — Paso 1 horse ──')\n",
    "study_cat_p1h = optuna.create_study(direction='maximize')\n",
    "study_cat_p1h.optimize(crear_objetivo_catboost(Xc_train, yc_train_p1_horse, spw_p1h, cat_features_idx), n_trials=N_TRIALS)\n",
    "print(f'Mejor CV F2: {study_cat_p1h.best_value:.4f}')\n",
    "\n",
    "print('\\n── Optuna CatBoost — Paso 1 prods ──')\n",
    "study_cat_p1p = optuna.create_study(direction='maximize')\n",
    "study_cat_p1p.optimize(crear_objetivo_catboost(Xc_train, yc_train_p1_prods, spw_p1p, cat_features_idx), n_trials=N_TRIALS)\n",
    "print(f'Mejor CV F2: {study_cat_p1p.best_value:.4f}')\n",
    "\n",
    "print('\\n── Optuna CatBoost — Paso 2 horse ──')\n",
    "study_cat_p2h = optuna.create_study(direction='maximize')\n",
    "study_cat_p2h.optimize(crear_objetivo_catboost(Xc_p2h, yc_p2h, spw_p2h, cat_features_idx), n_trials=N_TRIALS)\n",
    "print(f'Mejor CV F2: {study_cat_p2h.best_value:.4f}')\n",
    "\n",
    "print('\\n── Optuna CatBoost — Paso 2 prods ──')\n",
    "study_cat_p2p = optuna.create_study(direction='maximize')\n",
    "study_cat_p2p.optimize(crear_objetivo_catboost(Xc_p2p, yc_p2p, spw_p2p, cat_features_idx), n_trials=N_TRIALS)\n",
    "print(f'Mejor CV F2: {study_cat_p2p.best_value:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "v2_cd_04151",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entrenar_catboost_final(study, X_tr, y_tr, spw, cat_idx):\n",
    "    params = {**study.best_params, 'scale_pos_weight': spw,\n",
    "              'cat_features': cat_idx, 'random_seed': 42,\n",
    "              'verbose': 0, 'thread_count': -1, 'eval_metric': 'F1'}\n",
    "    model = CatBoostClassifier(**params)\n",
    "    model.fit(X_tr, y_tr)\n",
    "    return model\n",
    "\n",
    "cat_p1h = entrenar_catboost_final(study_cat_p1h, Xc_train, yc_train_p1_horse, spw_p1h, cat_features_idx)\n",
    "cat_p1p = entrenar_catboost_final(study_cat_p1p, Xc_train, yc_train_p1_prods, spw_p1p, cat_features_idx)\n",
    "cat_p2h = entrenar_catboost_final(study_cat_p2h, Xc_p2h,   yc_p2h,            spw_p2h, cat_features_idx)\n",
    "cat_p2p = entrenar_catboost_final(study_cat_p2p, Xc_p2p,   yc_p2p,            spw_p2p, cat_features_idx)\n",
    "\n",
    "# Para evaluar CatBoost usamos Xc_test\n",
    "def calcular_metricas_cat(p1h, p1p, p2h, p2p):\n",
    "    metricas = {}\n",
    "    for target, m1, m2, y_true, Xte_p2, yte_p2 in [\n",
    "        ('horse', p1h, p2h, yc_test['horse_target'],\n",
    "         Xc_test[mask_test_cat_horse],\n",
    "         (yc_test['horse_target'][mask_test_cat_horse]=='Lead Oro').astype(int)),\n",
    "        ('prods', p1p, p2p, yc_test['prods_target'],\n",
    "         Xc_test[mask_test_cat_prods],\n",
    "         (yc_test['prods_target'][mask_test_cat_prods]=='Lead Oro').astype(int)),\n",
    "    ]:\n",
    "        y_pred = predecir_cascada(Xc_test, m1, m2)\n",
    "        f2_macro = fbeta_score(y_true, y_pred, beta=2, average='macro', labels=orden_leads)\n",
    "        f2_oro   = fbeta_score(y_true, y_pred, beta=2, labels=['Lead Oro'], average='macro')\n",
    "        Xp2_tr = Xc_p2h if target=='horse' else Xc_p2p\n",
    "        yp2_tr = yc_p2h if target=='horse' else yc_p2p\n",
    "        f2_p2_tr = fbeta_score(yp2_tr, m2.predict(Xp2_tr), beta=2)\n",
    "        f2_p2_te = fbeta_score(yte_p2, m2.predict(Xte_p2), beta=2)\n",
    "        metricas[target] = {\n",
    "            'f2_macro': f2_macro, 'f2_lead_oro': f2_oro,\n",
    "            'f2_paso2_train': f2_p2_tr, 'f2_paso2_test': f2_p2_te,\n",
    "            'overfit_gap_p2': f2_p2_tr - f2_p2_te, 'y_pred': y_pred,\n",
    "        }\n",
    "    return metricas\n",
    "\n",
    "metricas_cat = calcular_metricas_cat(cat_p1h, cat_p1p, cat_p2h, cat_p2p)\n",
    "for t, m in metricas_cat.items():\n",
    "    print(f'{t}: F2 macro={m[\"f2_macro\"]:.4f} | F2 Oro={m[\"f2_lead_oro\"]:.4f} '\n",
    "          f'| Gap={m[\"overfit_gap_p2\"]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "v2_cd_48262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold tuning CatBoost\n",
    "def threshold_tuning_cat(m1, m2, Xte, y_true, target_name):\n",
    "    thresholds = np.arange(0.10, 0.70, 0.02)\n",
    "    resultados = []\n",
    "    for t in thresholds:\n",
    "        pred = np.array(['Lead Bronce'] * len(Xte), dtype=object)\n",
    "        mask_p1 = m1.predict_proba(Xte)[:, 1] >= 0.5\n",
    "        if mask_p1.sum() > 0:\n",
    "            proba_p2 = m2.predict_proba(Xte[mask_p1])[:, 1]\n",
    "            pred[mask_p1] = np.where(proba_p2 >= t, 'Lead Oro', 'Lead Plata')\n",
    "        f2_oro = fbeta_score(y_true, pred, beta=2, labels=['Lead Oro'], average='macro')\n",
    "        resultados.append({'threshold': t, 'f2_oro': f2_oro})\n",
    "    df_res = pd.DataFrame(resultados)\n",
    "    best = df_res.loc[df_res['f2_oro'].idxmax()]\n",
    "    print(f'{target_name}: óptimo={best[\"threshold\"]:.2f} | F2 Oro={best[\"f2_oro\"]:.4f}')\n",
    "    return float(best['threshold'])\n",
    "\n",
    "thresh_cat_horse = threshold_tuning_cat(cat_p1h, cat_p2h, Xc_test, yc_test['horse_target'], 'CatBoost horse')\n",
    "thresh_cat_prods = threshold_tuning_cat(cat_p1p, cat_p2p, Xc_test, yc_test['prods_target'], 'CatBoost prods')\n",
    "\n",
    "run_id_cat = loguear_run(\n",
    "    run_name   = 'v2_CatBoost_Optuna',\n",
    "    model_type = 'CatBoost',\n",
    "    params_p1  = {**study_cat_p1h.best_params, 'scale_pos_weight': spw_p1h},\n",
    "    params_p2  = {**study_cat_p2h.best_params, 'scale_pos_weight': spw_p2h,\n",
    "                  'threshold_horse': thresh_cat_horse, 'threshold_prods': thresh_cat_prods},\n",
    "    p1h=cat_p1h, p1p=cat_p1p,\n",
    "    p2h=cat_p2h, p2p=cat_p2p,\n",
    "    metricas=metricas_cat,\n",
    "    extra_metrics={'optuna_trials': N_TRIALS}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "v2_md_00388",
   "metadata": {},
   "source": [
    "## 6 · Comparativa v1 vs v2 y selección del campeón\n",
    "\n",
    "Comparamos todos los modelos de v2 entre sí y contra el campeón de v1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "v2_cd_08412",
   "metadata": {},
   "outputs": [],
   "source": [
    "CAMPEON_V1_F2_ORO = 0.4538  # XGB_Baseline v1\n",
    "\n",
    "todos_v2 = [\n",
    "    ('v2_XGB_Thresh',   metricas_xgb_thresh,   run_id_xgb_thresh),\n",
    "    ('v2_LightGBM',     metricas_lgbm_thresh,  run_id_lgbm),\n",
    "    ('v2_CatBoost',     metricas_cat,           run_id_cat),\n",
    "]\n",
    "\n",
    "print(f'{\"Modelo\":<20} {\"Target\":<8} {\"F2 macro\":>10} {\"F2 Oro\":>10} {\"Gap P2\":>8}  vs v1')\n",
    "print('═'*72)\n",
    "for nombre, res, _ in todos_v2:\n",
    "    for target in ['horse', 'prods']:\n",
    "        m = res[target]\n",
    "        delta = m['f2_lead_oro'] - CAMPEON_V1_F2_ORO\n",
    "        arrow = '↑' if delta > 0 else '↓'\n",
    "        print(f'{nombre:<20} {target:<8} {m[\"f2_macro\"]:>10.4f} {m[\"f2_lead_oro\"]:>10.4f} '\n",
    "              f'{m[\"overfit_gap_p2\"]:>8.4f}  {arrow}{abs(delta):.4f}')\n",
    "    print('─'*72)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "v2_cd_33208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selección automática del nuevo campeón\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "exp    = client.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "\n",
    "runs = client.search_runs(\n",
    "    experiment_ids=[exp.experiment_id],\n",
    "    filter_string='attributes.status = \"FINISHED\"',\n",
    "    order_by=['metrics.f2_lead_oro_horse DESC'],\n",
    "    max_results=20\n",
    ")\n",
    "\n",
    "campeon_actual = next(\n",
    "    (r for r in runs if r.data.tags.get('status') == 'champion'), None\n",
    ")\n",
    "mejor_run = runs[0]\n",
    "nombre_mejor = mejor_run.data.tags.get('mlflow.runName', mejor_run.info.run_id[:8])\n",
    "f2_mejor = mejor_run.data.metrics.get('f2_lead_oro_horse', 0)\n",
    "\n",
    "if campeon_actual and campeon_actual.info.run_id == mejor_run.info.run_id:\n",
    "    # El campeón actual ya es el mejor\n",
    "    print(f'{'═'*60}')\n",
    "    print(f'  CAMPEÓN VIGENTE: {nombre_mejor}')\n",
    "    print(f'  Ningún modelo de v2 superó el campeón actual.')\n",
    "    print(f'  F2 Lead Oro horse: {f2_mejor:.4f}')\n",
    "    print(f'{'═'*60}')\n",
    "else:\n",
    "    # Hay un nuevo campeón\n",
    "    client.set_tag(mejor_run.info.run_id, 'status', 'champion')\n",
    "    if campeon_actual:\n",
    "        client.set_tag(campeon_actual.info.run_id, 'status', 'retired')\n",
    "        nombre_anterior = campeon_actual.data.tags.get('mlflow.runName', campeon_actual.info.run_id[:8])\n",
    "        f2_anterior = campeon_actual.data.metrics.get('f2_lead_oro_horse', 0)\n",
    "        print(f'  Retirado: {nombre_anterior} (F2={f2_anterior:.4f})')\n",
    "    print(f'{'═'*60}')\n",
    "    print(f'  NUEVO CAMPEÓN: {nombre_mejor}')\n",
    "    print(f'  F2 Lead Oro horse: {f2_mejor:.4f}')\n",
    "    if campeon_actual:\n",
    "        print(f'  Mejora vs anterior: {f2_mejor - f2_anterior:+.4f}')\n",
    "    print(f'{'═'*60}')\n",
    "\n",
    "# Ranking completo\n",
    "print(f'\\n  Ranking completo:')\n",
    "print(f'  {\"Run\":<22} {\"F2 Oro horse\":>13} {\"Gap P2\":>8}  Status')\n",
    "print(f'  {\"─\"*55}')\n",
    "for r in runs:\n",
    "    rname  = r.data.tags.get('mlflow.runName', r.info.run_id[:8])\n",
    "    f2h    = r.data.metrics.get('f2_lead_oro_horse', float('nan'))\n",
    "    gap    = r.data.metrics.get('overfit_gap_p2_horse', float('nan'))\n",
    "    status = r.data.tags.get('status', '')\n",
    "    print(f'{rname:<20} {f2h:>13.4f} {gap:>8.4f}  {status}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "s02-26-e45-data-science-equinelead (3.12.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
